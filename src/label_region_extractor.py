"""
Label Region Extractor - Python Implementation
Chuyá»ƒn Ä‘á»•i tá»« C# LabelRegionExtractor.cs

Há»‡ thá»‘ng 3 táº§ng:
- Táº¦NG 1: Analysis (phÃ¢n tÃ­ch Ä‘á»™ tÆ°Æ¡ng pháº£n)
- Táº¦NG 2: Strategies (3 chiáº¿n lÆ°á»£c: HIGH/MEDIUM/LOW)
- Fallback Chain: MEDIUMâ†’HIGH, LOWâ†’MEDIUMâ†’HIGH
"""

import cv2
import numpy as np
from dataclasses import dataclass
from typing import Optional, Tuple
import matplotlib.pyplot as plt
from pathlib import Path
import json

# ============================================================================
# CONSTANTS (tá»« C# LabelRegionExtractor.cs)
# ============================================================================

# Analysis thresholds
HIGH_THRESHOLD = 0.45      # Ná»›i rá»™ng tá»« 0.6 â†’ 0.45
MEDIUM_THRESHOLD = 0.25    # Thu háº¹p tá»« 0.3 â†’ 0.25
EDGE_MAX = 0.1             # Normalization cho edge strength

# Label expansion ratios (cho Strategy LOW)
# QR náº±m á»Ÿ GIá»®A-PHáº¢I cá»§a nhÃ£n (cÃ³ padding cáº¡nh pháº£i vÃ  dÆ°á»›i)
# CÃ¡c ratios nÃ y chá»‰ dÃ¹ng cho tham kháº£o, logic thá»±c táº¿ dÃ¹ng expansion factors bÃªn dÆ°á»›i
LABEL_WIDTH_RATIO = 4.7     # NhÃ£n rá»™ng â‰ˆ 4.7Ã— QR (3.5 trÃ¡i + 1.0 QR + 0.2 pháº£i = 4.7)
LABEL_HEIGHT_RATIO = 3.2    # NhÃ£n cao â‰ˆ 3.2Ã— QR (1.0 trÃªn + 1.0 QR + 1.2 dÆ°á»›i = 3.2)

# Expansion factors (dá»±a trÃªn vá»‹ trÃ­ CENTER-RIGHT)
QR_VERTICAL_CENTER_UP = 1.0      # Má»Ÿ rá»™ng 1.0Ã— lÃªn trÃªn
QR_VERTICAL_CENTER_DOWN = 1.2    # Má»Ÿ rá»™ng 1.2Ã— xuá»‘ng dÆ°á»›i (thÃªm 0.2Ã— padding)
QR_HORIZONTAL_RIGHT = 0.2        # Má»Ÿ rá»™ng 0.2Ã— sang pháº£i (thÃªm padding pháº£i)
QR_LEFT_EXPANSION = 3.5          # Má»Ÿ rá»™ng 3.5Ã— sang trÃ¡i

# Padding Ä‘á»ƒ trÃ¡nh cáº¯t nháº§m (tÃ¹y chá»n)
PADDING_RATIO = 0.2              # 20% padding chung cho táº¥t cáº£ cÃ¡c cáº¡nh (dá»±a trÃªn kÃ­ch thÆ°á»›c QR)

# Debug output directory
DEBUG_OUTPUT_DIR = "data/debug"


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_analysis_region(gray: np.ndarray) -> np.ndarray:
    """
    Láº¥y vÃ¹ng Ä‘á»ƒ phÃ¢n tÃ­ch.
    
    TrÆ°á»›c: Láº¥y 1/3 center (vÃ¬ áº£nh lá»›n, nhÃ£n á»Ÿ giá»¯a)
    Hiá»‡n táº¡i: Láº¥y toÃ n bá»™ áº£nh (vÃ¬ input Ä‘Ã£ lÃ  vÃ¹ng nhá» chá»©a nhÃ£n)
    
    Args:
        gray: áº¢nh grayscale
    
    Returns:
        Region Ä‘á»ƒ phÃ¢n tÃ­ch (toÃ n bá»™ áº£nh)
    """
    return gray


def save_debug_image(image: np.ndarray, filename: str, cmap='gray'):
    """
    LÆ°u áº£nh debug.
    
    Args:
        image: áº¢nh cáº§n lÆ°u
        filename: TÃªn file (sáº½ tá»± Ä‘á»™ng thÃªm prefix debug_)
        cmap: Colormap cho matplotlib
    """
    output_dir = Path(DEBUG_OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = output_dir / f"debug_{filename}"
    
    if len(image.shape) == 2:  # Grayscale
        plt.figure(figsize=(10, 6))
        plt.imshow(image, cmap=cmap)
        plt.colorbar()
        plt.title(filename)
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
    else:  # Color (BGR -> RGB)
        plt.figure(figsize=(10, 6))
        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        plt.title(filename)
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
    
    print(f"  ğŸ’¾ Saved debug: {output_path}")


def save_debug_json(data: dict, filename: str):
    """
    LÆ°u dá»¯ liá»‡u debug dáº¡ng JSON.
    
    Args:
        data: Dictionary chá»©a dá»¯ liá»‡u
        filename: TÃªn file (sáº½ tá»± Ä‘á»™ng thÃªm prefix debug_)
    """
    output_dir = Path(DEBUG_OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_path = output_dir / f"debug_{filename}"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"  ğŸ’¾ Saved debug: {output_path}")


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class ContrastAnalysisResult:
    """Káº¿t quáº£ phÃ¢n tÃ­ch Ä‘á»™ tÆ°Æ¡ng pháº£n."""
    level: str  # 'High', 'Medium', 'Low'
    final_score: float
    
    # 3 metrics
    separation: float
    edge_strength: float
    contrast_ratio: float
    
    # Debug info
    peak1_position: int
    peak2_position: int
    edge_pixel_count: int
    mean_intensity: float
    stddev_intensity: float


# ============================================================================
# Táº¦NG 1: ANALYSIS METHODS
# ============================================================================

def analyze_histogram(gray: np.ndarray) -> Tuple[int, int, float]:
    """
    PhÃ¢n tÃ­ch histogram Ä‘á»ƒ tÃ¬m 2 peaks chÃ­nh vÃ  tÃ­nh separation.
    
    Logic:
    1. Láº¥y vÃ¹ng phÃ¢n tÃ­ch (toÃ n bá»™ áº£nh)
    2. TÃ­nh histogram 256 bins
    3. Smooth báº±ng moving average 5 bins
    4. TÃ¬m local maxima (> 0.5 avgHeight, cÃ¡ch nhau >30 bins)
    5. Chá»n 2 peaks cao nháº¥t, sort theo position
    6. separation = |peak2 - peak1| / 255.0
    
    Returns:
        (peak1_pos, peak2_pos, separation)
    """
    # 1. Láº¥y vÃ¹ng phÃ¢n tÃ­ch
    analysis_roi = get_analysis_region(gray)
    
    # 2. Histogram
    hist = cv2.calcHist([analysis_roi], [0], None, [256], [0, 256])
    hist = hist.flatten()
    
    # 3. Smooth (moving average 5 bins)
    smoothed = np.copy(hist)
    for i in range(2, 254):
        smoothed[i] = np.mean(hist[i-2:i+3])
    
    # 4. Find local maxima
    avg_height = np.mean(smoothed)
    threshold = avg_height * 0.5
    
    peaks = []
    for i in range(10, 246):
        is_local_max = (smoothed[i] > smoothed[i-1] and 
                       smoothed[i] > smoothed[i+1] and 
                       smoothed[i] > threshold)
        
        if is_local_max:
            # Check not too close to existing peaks
            too_close = any(abs(p[0] - i) < 30 for p in peaks)
            if not too_close:
                peaks.append((i, smoothed[i]))
    
    # 5. Take 2 highest peaks
    if len(peaks) < 2:
        peak1, peak2, separation = 0, 255, 0.0
    else:
        peaks = sorted(peaks, key=lambda x: x[1], reverse=True)[:2]
        peaks = sorted(peaks, key=lambda x: x[0])  # Sort by position
        
        peak1 = peaks[0][0]
        peak2 = peaks[1][0]
        separation = abs(peak2 - peak1) / 255.0
    
    # Debug: Váº½ histogram vá»›i peaks (luÃ´n váº½ dÃ¹ cÃ³ peaks hay khÃ´ng)
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(hist, color='gray', alpha=0.5, label='Original')
    plt.plot(smoothed, color='blue', label='Smoothed')
    plt.axvline(peak1, color='red', linestyle='--', label=f'Peak1={peak1}')
    plt.axvline(peak2, color='green', linestyle='--', label=f'Peak2={peak2}')
    plt.title(f'Histogram Analysis (Separation={separation:.3f})')
    plt.xlabel('Intensity')
    plt.ylabel('Frequency')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.imshow(analysis_roi, cmap='gray')
    plt.title('Analysis Region')
    plt.axis('off')
    
    plt.tight_layout()
    output_path = Path(DEBUG_OUTPUT_DIR) / "debug_01_histogram.png"
    Path(DEBUG_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  ğŸ’¾ Saved debug: {output_path}")
    
    return (peak1, peak2, separation)


def analyze_edges(gray: np.ndarray) -> Tuple[int, float]:
    """
    PhÃ¢n tÃ­ch edges báº±ng Canny Ä‘á»ƒ tÃ­nh edge strength.
    
    Logic:
    1. Láº¥y vÃ¹ng phÃ¢n tÃ­ch (toÃ n bá»™ áº£nh)
    2. Canny(50, 150)
    3. Äáº¿m non-zero pixels
    4. edge_strength = edge_pixels / total_pixels
    
    Returns:
        (edge_pixels, edge_strength)
    """
    # 1. Láº¥y vÃ¹ng phÃ¢n tÃ­ch
    analysis_roi = get_analysis_region(gray)
    
    # 2. Canny Edge Detection
    edges = cv2.Canny(analysis_roi, threshold1=50, threshold2=150)
    
    # 3. Count edge pixels
    edge_pixels = cv2.countNonZero(edges)
    total_pixels = analysis_roi.shape[0] * analysis_roi.shape[1]
    
    edge_strength = edge_pixels / total_pixels
    
    # Debug: LÆ°u áº£nh edges
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(analysis_roi, cmap='gray')
    plt.title('Original Image')
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    plt.imshow(edges, cmap='gray')
    plt.title(f'Canny Edges (Strength={edge_strength:.4f}, {edge_pixels} pixels)')
    plt.axis('off')
    
    plt.tight_layout()
    output_path = Path(DEBUG_OUTPUT_DIR) / "debug_02_edges.png"
    Path(DEBUG_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  ğŸ’¾ Saved debug: {output_path}")
    
    return (edge_pixels, edge_strength)


def analyze_contrast(gray: np.ndarray) -> Tuple[float, float, float]:
    """
    PhÃ¢n tÃ­ch contrast báº±ng standard deviation.
    
    Logic:
    1. Láº¥y vÃ¹ng phÃ¢n tÃ­ch (toÃ n bá»™ áº£nh)
    2. TÃ­nh mean, stddev
    3. contrast_ratio = stddev / 128.0
    
    Returns:
        (mean, stddev, contrast_ratio)
    """
    # 1. Láº¥y vÃ¹ng phÃ¢n tÃ­ch
    analysis_roi = get_analysis_region(gray)
    
    # 2. Calculate mean and stddev
    mean, stddev = cv2.meanStdDev(analysis_roi)
    mean = mean[0][0]
    stddev = stddev[0][0]
    
    # 3. Contrast ratio
    contrast_ratio = stddev / 128.0
    
    # Debug: LÆ°u JSON vÃ  visualization
    contrast_data = {
        "mean_intensity": float(mean),
        "stddev_intensity": float(stddev),
        "contrast_ratio": float(contrast_ratio),
        "min_intensity": float(np.min(analysis_roi)),
        "max_intensity": float(np.max(analysis_roi)),
        "image_shape": list(analysis_roi.shape)
    }
    
    save_debug_json(contrast_data, "03_contrast.json")
    
    # Váº½ phÃ¢n bá»‘ cÆ°á»ng Ä‘á»™
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.imshow(analysis_roi, cmap='gray')
    plt.title(f'Analysis Region (Mean={mean:.1f}, StdDev={stddev:.1f})')
    plt.colorbar()
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    plt.hist(analysis_roi.ravel(), bins=256, range=(0, 256), color='blue', alpha=0.7)
    plt.axvline(mean, color='red', linestyle='--', label=f'Mean={mean:.1f}')
    plt.axvline(mean - stddev, color='orange', linestyle=':', label=f'Mean-Ïƒ={mean-stddev:.1f}')
    plt.axvline(mean + stddev, color='orange', linestyle=':', label=f'Mean+Ïƒ={mean+stddev:.1f}')
    plt.title(f'Intensity Distribution (Contrast Ratio={contrast_ratio:.3f})')
    plt.xlabel('Intensity')
    plt.ylabel('Frequency')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = Path(DEBUG_OUTPUT_DIR) / "debug_03_contrast.png"
    Path(DEBUG_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  ğŸ’¾ Saved debug: {output_path}")
    
    return (mean, stddev, contrast_ratio)


def analyze_frame(gray: np.ndarray) -> ContrastAnalysisResult:
    """
    PhÃ¢n tÃ­ch frame Ä‘á»ƒ tÃ­nh final score vÃ  xÃ¡c Ä‘á»‹nh contrast level.
    
    Logic tá»« C#:
    1. Gá»i 3 hÃ m phÃ¢n tÃ­ch
    2. Normalize edge_strength (min(edge_strength / 0.1, 1.0))
    3. Final Score = separationx0.4 + edge_strength_normx0.3 + contrast_ratiox0.3
    4. PhÃ¢n loáº¡i: >0.45=High, 0.25-0.45=Medium, <0.25=Low
    
    Returns:
        ContrastAnalysisResult
    """
    # 1. Call 3 analysis methods
    peak1, peak2, separation = analyze_histogram(gray)
    edge_pixels, edge_strength = analyze_edges(gray)
    mean, stddev, contrast_ratio = analyze_contrast(gray)
    
    # 2. Normalize edge strength
    edge_strength_norm = min(edge_strength / EDGE_MAX, 1.0)
    
    # 3. Calculate Final Score
    final_score = (separation * 0.4 + 
                   edge_strength_norm * 0.3 + 
                   contrast_ratio * 0.3)
    
    # ============================================================
    # ğŸ”§ DEBUG MODE: FORCE LOW STRATEGY
    # Táº¡m thá»i force final_score = 0.01 Ä‘á»ƒ luÃ´n cháº¡y LOW strategy
    # TODO: XÃ³a dÃ²ng nÃ y sau khi debug xong!
    # ============================================================
    final_score = 0.01
    print("  âš ï¸ DEBUG MODE: Forcing LOW strategy (final_score = 0.01)")
    # ============================================================
    
    # 4. Determine level
    if final_score > HIGH_THRESHOLD:
        level = 'High'
    elif final_score > MEDIUM_THRESHOLD:
        level = 'Medium'
    else:
        level = 'Low'
    
    # 5. Return result
    return ContrastAnalysisResult(
        level=level,
        final_score=final_score,
        separation=separation,
        edge_strength=edge_strength,
        contrast_ratio=contrast_ratio,
        peak1_position=peak1,
        peak2_position=peak2,
        edge_pixel_count=edge_pixels,
        mean_intensity=mean,
        stddev_intensity=stddev
    )


# ============================================================================
# Táº¦NG 2: STRATEGY METHODS
# ============================================================================

def detect_with_high_contrast(src: np.ndarray, gray: np.ndarray, 
                             threshold_value: int = 150) -> Tuple:
    """
    Strategy HIGH: Binary Threshold + Morphology (cho Ã¡o tá»‘i/mÃ u Ä‘áº­m).
    
    Logic tá»« C# (Ä‘Ã£ cáº­p nháº­t):
    1. Otsu adaptive threshold thay vÃ¬ hardcoded value
    2. Morphology: Open(3x3, 1 iter) â†’ Close(3x3, 2 iters)
    3. FindContours(EXTERNAL)
    4. Chá»n largest contour theo area
    5. MinAreaRect â†’ box
    6. Crop bounding rect â†’ QR verification
    7. Tráº£ vá» (rect, box, qr_text, qr_points_180, qr_points)
    
    Returns:
        tuple: (rect, box, qr_text, qr_points_180, qr_points) or (None, None, None, None, None)
    """
    print("  â†’ Method: Binary Threshold + Morphology")
    
    # 1. Otsu adaptive threshold
    otsu_threshold, binary = cv2.threshold(gray, 0, 255, 
                                          cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    print(f"  â†’ Otsu adaptive threshold: {otsu_threshold:.1f} (auto-calculated)")
    
    # 2. Morphology
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    morph = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)
    morph = cv2.morphologyEx(morph, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    # 3. Find contours
    contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, 
                                   cv2.CHAIN_APPROX_SIMPLE)
    
    if len(contours) == 0:
        print("  âœ— No contours found")
        return (None, None, None, None, None)
    
    print(f"  â†’ Found {len(contours)} contours")
    
    # 4. Find largest contour
    biggest = max(contours, key=cv2.contourArea)
    max_area = cv2.contourArea(biggest)
    print(f"  â†’ Largest contour area: {max_area:.0f} pixels")
    
    # 5. MinAreaRect
    rect = cv2.minAreaRect(biggest)
    box = cv2.boxPoints(rect)
    box = np.int32(box)
    
    # 6. Crop and verify QR
    bound = cv2.boundingRect(biggest)
    x, y, w, h = bound
    
    # Clamp to image bounds
    x = max(0, x)
    y = max(0, y)
    w = min(src.shape[1] - x, w)
    h = min(src.shape[0] - y, h)
    
    label_roi = src[y:y+h, x:x+w]
    
    # QR detection
    qr_detector = cv2.QRCodeDetector()
    qr_text, qr_points_180, _ = qr_detector.detectAndDecode(label_roi)
    
    qr_points = None
    if qr_points_180 is not None and len(qr_points_180) > 0:
        # Reshape if needed (sometimes shape is (1, 4, 2))
        if qr_points_180.ndim == 3:
            qr_points_180 = qr_points_180.reshape(-1, 2)
        
        # Convert to global coordinates
        qr_points = qr_points_180.copy()
        qr_points[:, 0] += x
        qr_points[:, 1] += y
    
    if qr_text:
        print(f"  âœ“ QR detected: {qr_text}")
        return (rect, box, qr_text, qr_points_180, qr_points)
    
    print("  âœ— No QR code found in label region")
    return (None, None, None, None, None)


def detect_with_medium_contrast(src: np.ndarray, gray: np.ndarray) -> Tuple:
    """
    Strategy MEDIUM: Canny Edge + Strong Morphology (cho Ã¡o mÃ u nháº¡t).
    
    Logic tá»« C# (Ä‘Ã£ cáº­p nháº­t):
    1. Canny(30, 100) - Lower thresholds Ä‘á»ƒ nháº¡y hÆ¡n
    2. Morphology: Close(7x7, 3 iters) â†’ Dilate(7x7, 1 iter)
    3. FindContours(EXTERNAL)
    4. Filter CHá»ˆ theo area ratio (5-80%)
    5. Sort theo area (lá»›n nháº¥t trÆ°á»›c)
    6. Loop candidates â†’ verify QR â†’ Early exit khi tÃ¬m tháº¥y
    
    Returns:
        tuple: (rect, box, qr_text, qr_points_180, qr_points) or (None, None, None, None, None)
    """
    print("  â†’ Method: Canny Edge + Strong Morphology")
    
    # 1. Canny with lower thresholds
    edges = cv2.Canny(gray, threshold1=30, threshold2=100)
    print("  â†’ Lower Canny thresholds (30/100) for better edge detection")
    
    # 2. Strong morphology
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))
    edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=3)
    edges = cv2.dilate(edges, kernel, iterations=1)
    
    # 3. Find contours
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, 
                                   cv2.CHAIN_APPROX_SIMPLE)
    
    if len(contours) == 0:
        print("  âœ— No contours found")
        return (None, None, None, None, None)
    
    print(f"  â†’ Found {len(contours)} contours")
    
    # 4. Filter by area ratio (5-80%)
    roi_area = gray.shape[0] * gray.shape[1]
    candidates = []
    
    for c in contours:
        area = cv2.contourArea(c)
        area_ratio = area / roi_area
        
        if 0.05 <= area_ratio <= 0.80:
            rect = cv2.minAreaRect(c)
            candidates.append((c, rect, area))
    
    # 5. Sort by area (largest first)
    candidates = sorted(candidates, key=lambda x: x[2], reverse=True)
    print(f"  â†’ {len(candidates)} candidates after filtering (area 5-80%)")
    
    # 6. Loop and verify QR (Early Exit)
    for contour, rect, area in candidates:
        bound = cv2.boundingRect(contour)
        x, y, w, h = bound
        
        # Clamp to bounds
        x = max(0, x)
        y = max(0, y)
        w = min(src.shape[1] - x, w)
        h = min(src.shape[0] - y, h)
        
        if w <= 0 or h <= 0:
            continue
        
        label_roi = src[y:y+h, x:x+w]
        
        # QR detection
        qr_detector = cv2.QRCodeDetector()
        qr_text, qr_points_180, _ = qr_detector.detectAndDecode(label_roi)
        
        qr_points = None
        if qr_points_180 is not None and len(qr_points_180) > 0:
            if qr_points_180.ndim == 3:
                qr_points_180 = qr_points_180.reshape(-1, 2)
            
            qr_points = qr_points_180.copy()
            qr_points[:, 0] += x
            qr_points[:, 1] += y
        
        if qr_text:
            box = cv2.boxPoints(rect)
            box = np.int32(box)
            print(f"  âœ“ QR detected in candidate (area={area:.0f}): {qr_text}")
            return (rect, box, qr_text, qr_points_180, qr_points)
    
    print("  âœ— No QR found in any candidate")
    return (None, None, None, None, None)


def debug_low_strategy_geometry(src: np.ndarray, qr_points: np.ndarray, 
                                box: np.ndarray, p1: np.ndarray, p2: np.ndarray,
                                label_top_right: np.ndarray, label_top_left: np.ndarray,
                                expansion_up: float, expansion_left: float):
    """
    Debug visualization cho LOW strategy geometry.
    Váº½ QR box, label box, vÃ  cÃ¡c vectors má»Ÿ rá»™ng.
    
    Args:
        src: áº¢nh gá»‘c
        qr_points: 4 Ä‘iá»ƒm QR code
        box: 4 gÃ³c label Ä‘Ã£ tÃ­nh
        p1, p2: Äiá»ƒm QR top-right vÃ  bottom-right
        label_top_right, label_top_left: GÃ³c label
        expansion_up, expansion_left: Khoáº£ng má»Ÿ rá»™ng
    """
    debug_vis = src.copy()
    
    # Váº½ QR box (Ä‘á»)
    qr_box_int = qr_points.astype(np.int32)
    cv2.polylines(debug_vis, [qr_box_int], True, (0, 0, 255), 2)
    
    # Váº½ label box (xanh lÃ¡)
    cv2.polylines(debug_vis, [box], True, (0, 255, 0), 3)
    
    # Váº½ 4 gÃ³c QR (Ä‘á»)
    for i, pt in enumerate(qr_points):
        pt_int = tuple(pt.astype(int))
        cv2.circle(debug_vis, pt_int, 5, (0, 0, 255), -1)
        cv2.putText(debug_vis, f"QR{i}", (pt_int[0] + 10, pt_int[1] + 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    
    # Váº½ 4 gÃ³c label (xanh lÃ¡)
    label_names = ["L0:TL", "L1:TR", "L2:BR", "L3:BL"]  # TL=top-left, TR=top-right, etc.
    for i, pt in enumerate(box):
        cv2.circle(debug_vis, tuple(pt), 8, (0, 255, 0), -1)
        cv2.putText(debug_vis, label_names[i], (pt[0] + 10, pt[1] + 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    # Váº½ expansion vectors (mÃ u vÃ ng)
    # p1 -> label_top_right (expand UP)
    cv2.arrowedLine(debug_vis, tuple(p1.astype(int)), 
                   tuple(label_top_right.astype(int)), (0, 255, 255), 2)
    mid_pt = ((p1 + label_top_right) / 2).astype(int)
    cv2.putText(debug_vis, f"up:{expansion_up:.0f}px", tuple(mid_pt), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
    
    # label_top_right -> label_top_left (expand LEFT)
    cv2.arrowedLine(debug_vis, tuple(label_top_right.astype(int)), 
                   tuple(label_top_left.astype(int)), (255, 255, 0), 2)
    mid_pt = ((label_top_right + label_top_left) / 2).astype(int)
    cv2.putText(debug_vis, f"left:{expansion_left:.0f}px", tuple(mid_pt), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
    
    # ThÃªm text thÃ´ng tin
    info_y = 30
    cv2.putText(debug_vis, "LOW Strategy Geometry Debug", (10, info_y), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 255), 2)
    info_y += 30
    cv2.putText(debug_vis, "Red: QR box | Green: Label box", (10, info_y), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
    
    save_debug_image(debug_vis, "05_low_geometry_debug.png")
    print(f"  ğŸ’¾ Debug geometry visualization saved")


def try_detect_qr_multiple_methods(src: np.ndarray, gray: np.ndarray) -> Tuple:
    """
    Thá»­ detect QR code vá»›i 3 phÆ°Æ¡ng phÃ¡p preprocessing khÃ¡c nhau.
    Return ngay khi tÃ¬m tháº¥y (early exit).
    
    Methods (theo thá»© tá»± Æ°u tiÃªn):
    1. Gray (CLAHE) - ÄÃ£ qua CLAHE preprocessing, tÄƒng contrast cá»¥c bá»™
    2. Histogram Equalization - TÄƒng contrast toÃ n cá»¥c, "siÃªu tÆ°Æ¡ng pháº£n"
    3. Original BGR - áº¢nh gá»‘c, khÃ´ng xá»­ lÃ½ (fallback cuá»‘i cÃ¹ng)
    
    Args:
        src: áº¢nh BGR gá»‘c
        gray: áº¢nh grayscale Ä‘Ã£ qua CLAHE
    
    Returns:
        (qr_text, qr_points, method_name) or (None, None, None)
    """
    qr_detector = cv2.QRCodeDetector()
    
    # Danh sÃ¡ch cÃ¡c methods Ä‘á»ƒ thá»­
    methods = []
    
    # Method 1: Gray (CLAHE) - ÄÃ£ Ä‘Æ°á»£c apply CLAHE á»Ÿ hÃ m cha
    # ÄÃ¢y lÃ  best candidate vÃ¬ CLAHE tÄƒng contrast cá»¥c bá»™ mÃ  khÃ´ng lÃ m mÃ©o
    methods.append(("gray_clahe", gray))
    
    # Method 2: Histogram Equalization - "SiÃªu tÆ°Æ¡ng pháº£n"
    # TÄƒng contrast toÃ n cá»¥c máº¡nh, hiá»‡u quáº£ nhÆ°ng cÃ³ thá»ƒ mÃ©o QR
    enhanced = cv2.equalizeHist(gray)
    methods.append(("hist_equal", enhanced))
    
    # Method 3: Original BGR - áº¢nh gá»‘c
    # Fallback cuá»‘i cÃ¹ng, Ä‘Ã´i khi má»i preprocessing Ä‘á»u fail mÃ  BGR láº¡i work
    methods.append(("original_bgr", src))
    
    # Thá»­ tá»«ng method, return ngay khi tÃ¬m tháº¥y
    print("  â†’ Trying QR detection with multiple preprocessing methods...")
    for method_name, img in methods:
        qr_text, qr_points, _ = qr_detector.detectAndDecode(img)
        
        # Debug: In chi tiáº¿t káº¿t quáº£ detect
        print(f"     â€¢ Method '{method_name}': text={repr(qr_text)}, points_shape={qr_points.shape if qr_points is not None else 'None'}")
        
        # Check cÃ³ detect Ä‘Æ°á»£c khÃ´ng
        has_text = qr_text and len(qr_text) > 0
        has_points = qr_points is not None and qr_points.size > 0
        
        if has_text and has_points:
            # Reshape náº¿u cáº§n
            if qr_points.ndim == 3:
                qr_points = qr_points.reshape(-1, 2)
            
            if len(qr_points) >= 4:
                print(f"  âœ“ QR detected with method: {method_name}")
                # LÆ°u method thÃ nh cÃ´ng
                save_debug_image(img, f"04_low_qr_success_{method_name}.png", 
                               cmap='gray' if len(img.shape) == 2 else None)
                return qr_text, qr_points, method_name
            else:
                print(f"     âœ— Points count too low: {len(qr_points)}")
    
    # Táº¥t cáº£ methods Ä‘á»u fail
    print("  âœ— QR detection failed with all methods")
    
    # LÆ°u táº¥t cáº£ failed attempts Ä‘á»ƒ debug
    for method_name, img in methods:
        save_debug_image(img, f"04_low_qr_failed_{method_name}.png", 
                       cmap='gray' if len(img.shape) == 2 else None)
    
    return None, None, None


def detect_with_low_contrast(src: np.ndarray, gray: np.ndarray) -> Tuple:
    """
    Strategy LOW: QR-First + Geometry Inference (cho Ã¡o tráº¯ng/kem).
    
    Logic tá»« C# (Ä‘Ã£ cáº­p nháº­t - CENTER-RIGHT positioning):
    1. Multi-method QR detection (gray_clahe â†’ hist_equal â†’ original_bgr)
    2. TÃ­nh geometry QR (vectors, width, height, angle)
    3. Suy luáº­n label vá»›i expansion ratios:
       - Chiá»u cao: 2.0Ã— QR (QR á»Ÿ giá»¯a â†’ má»Ÿ rá»™ng 0.5Ã— lÃªn/xuá»‘ng)
       - Chiá»u rá»™ng: 4.0Ã— QR (QR á»Ÿ pháº£i â†’ má»Ÿ rá»™ng 3.0Ã— sang trÃ¡i)
    4. Construct 4 corners (QR á»Ÿ GIá»®A-PHáº¢I, expand TRÃI + TRÃŠN + DÆ¯á»šI)
    5. Táº¡o RotatedRect tá»« 4 corners
    
    Returns:
        tuple: (rect, box, qr_text, None, qr_points)
        Note: qr_points_180 = None vÃ¬ detect trÃªn toÃ n áº£nh
    """
    print("  â†’ Method: QR-First + Geometry Inference")
    
    # Debug: LÆ°u áº£nh input
    save_debug_image(gray, "04_low_input_gray.png", cmap='gray')
    save_debug_image(src, "04_low_input_src.png")
    
    # 1. Detect QR vá»›i multiple methods
    qr_text, qr_points, method_used = try_detect_qr_multiple_methods(src, gray)
    
    if not qr_text or qr_points is None or len(qr_points) < 4:
        print("  âœ— No QR code detected")
        return (None, None, None, None, None)
    
    # Reshape if needed
    if qr_points.ndim == 3:
        qr_points = qr_points.reshape(-1, 2)
    
    print(f"  âœ“ QR detected: {qr_text}")
    
    # 3. Calculate QR geometry
    p0 = qr_points[0]  # top-left
    p1 = qr_points[1]  # top-right
    p3 = qr_points[3]  # bottom-left
    
    top_vec = p1 - p0
    left_vec = p3 - p0
    
    qr_width = np.linalg.norm(top_vec)
    qr_height = np.linalg.norm(left_vec)
    
    # Unit vectors
    dir_right = top_vec / qr_width
    dir_down = left_vec / qr_height
    
    angle_rad = np.arctan2(top_vec[1], top_vec[0])
    angle_deg = angle_rad * 180.0 / np.pi
    print(f"  â†’ QR geometry: {qr_width:.1f}x{qr_height:.1f} px, angle={angle_deg:.1f}Â°")
    
    # 4. Infer label dimensions and expansion distances
    # QR á»Ÿ GIá»®A-PHáº¢I cá»§a nhÃ£n â†’ má»Ÿ rá»™ng TRÃI, LÃŠN TRÃŠN, XUá»NG DÆ¯á»šI, PHáº¢I
    
    # TÃ­nh Ä‘iá»ƒm QR bottom-right (p2)
    p2 = p3 + (dir_right * qr_width)  # QR bottom-right
    
    # TÃ­nh padding chung (Ã¡p dá»¥ng cho táº¥t cáº£ cÃ¡c cáº¡nh)
    padding_h = qr_height * PADDING_RATIO  # Padding dá»c (10% QR height)
    padding_w = qr_width * PADDING_RATIO   # Padding ngang (10% QR width)
    
    # TÃ­nh khoáº£ng má»Ÿ rá»™ng (bao gá»“m padding chung)
    expansion_up = qr_height * QR_VERTICAL_CENTER_UP + padding_h        # 1.0Ã— + padding
    expansion_down = qr_height * QR_VERTICAL_CENTER_DOWN + padding_h    # 1.2Ã— + padding
    expansion_left = qr_width * QR_LEFT_EXPANSION + padding_w           # 3.5Ã— + padding
    expansion_right = qr_width * QR_HORIZONTAL_RIGHT + padding_w        # 0.2Ã— + padding
    
    # TÃ­nh label dimensions
    label_width = expansion_left + qr_width + expansion_right
    label_height = expansion_up + qr_height + expansion_down
    
    print(f"  â†’ Predicted label: {label_width:.1f}x{label_height:.1f} px")
    print(f"  â†’ Base expansion: â†‘{QR_VERTICAL_CENTER_UP}Ã—QR, â†“{QR_VERTICAL_CENTER_DOWN}Ã—QR, â†{QR_LEFT_EXPANSION}Ã—QR, â†’{QR_HORIZONTAL_RIGHT}Ã—QR")
    print(f"  â†’ Padding: {PADDING_RATIO}Ã—QR = Â±{padding_h:.1f}px (vertical), Â±{padding_w:.1f}px (horizontal)")
    print(f"  â†’ Final expansion: â†‘{expansion_up:.1f}px, â†“{expansion_down:.1f}px, â†{expansion_left:.1f}px, â†’{expansion_right:.1f}px")
    
    # 5. Calculate 4 corners
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â† Label top (1.0Ã—QR + 0.1Ã—padding)
    # â”‚                        â”Œâ”€â”€â”€â”€â”       â”‚
    # â”‚                        â”‚ QR â”‚ â†â”€â”€â”€â”€â”€â”¤  QR gáº§n pháº£i (0.2Ã—QR + 0.1Ã—padding)
    # â”‚                        â””â”€â”€â”€â”€â”˜       â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â† Label bottom (1.2Ã—QR + 0.1Ã—padding)
    # â†‘                                     â†‘
    # TrÃ¡i: 3.5Ã—QR + 0.1Ã—padding             Pháº£i: 0.2Ã—QR + 0.1Ã—padding
    
    # TÃ­nh 4 gÃ³c nhÃ£n
    # 1. Label TOP-RIGHT: tá»« QR top-right (p1) Ä‘i lÃªn rá»“i sang pháº£i
    label_top_right = p1 - (dir_down * expansion_up) + (dir_right * expansion_right)
    
    # 2. Label BOTTOM-RIGHT: tá»« QR bottom-right (p2) Ä‘i xuá»‘ng rá»“i sang pháº£i
    label_bottom_right = p2 + (dir_down * expansion_down) + (dir_right * expansion_right)
    
    # 3. Label TOP-LEFT: tá»« top-right Ä‘i sang trÃ¡i
    label_top_left = label_top_right - (dir_right * label_width)
    
    # 4. Label BOTTOM-LEFT: tá»« bottom-right Ä‘i sang trÃ¡i
    label_bottom_left = label_bottom_right - (dir_right * label_width)
    
    # 6. Create RotatedRect
    label_center = (label_top_left + label_top_right + 
                   label_bottom_right + label_bottom_left) / 4.0
    
    angle = angle_deg
    
    # RotatedRect as tuple (OpenCV format)
    rect = (tuple(label_center), (label_width, label_height), angle)
    
    box = np.array([label_top_left, label_top_right, 
                    label_bottom_right, label_bottom_left], dtype=np.int32)
    
    print(f"  âœ“ Label constructed: center=({label_center[0]:.1f},{label_center[1]:.1f}), angle={angle:.1f}Â°")
    
    # Debug: Váº½ geometry visualization
    debug_low_strategy_geometry(src, qr_points, box, p1, p2, 
                               label_top_right, label_top_left,
                               expansion_up, expansion_left)
    
    # qr_points_180 = None (detect trÃªn toÃ n áº£nh, khÃ´ng cÃ³ ROI cá»¥c bá»™)
    return (rect, box, qr_text, None, qr_points)


# ============================================================================
# HÃ€M CHÃNH: DETECT LABEL REGION
# ============================================================================

def detect_label_region(src: np.ndarray, 
                       threshold_value: int = 150) -> Tuple[Optional[Tuple], 
                                                            Optional[np.ndarray], 
                                                            Optional[str],
                                                            Optional[np.ndarray],
                                                            Optional[np.ndarray],
                                                            Optional[str]]:
    """
    PhÃ¡t hiá»‡n vÃ¹ng nhÃ£n trong áº£nh.
    
    Logic tá»« C# (flow chÃ­nh xÃ¡c):
    1. Preprocessing: BGR â†’ Gray â†’ GaussianBlur(5x5)
    2. Táº¦NG 1: PhÃ¢n tÃ­ch (AnalyzeFrame) â†’ ContrastAnalysisResult
    3. Log analysis metrics
    4. Táº¦NG 2: Routing theo level:
       - HIGH: DetectWithHighContrast()
       - MEDIUM: DetectWithMediumContrast() â†’ fallback to HIGH
       - LOW: Apply CLAHE â†’ DetectWithLowContrast() â†’ fallback chain (â†’MEDIUMâ†’HIGH)
    5. Tráº£ vá» (rect, box, qr_text, qr_points_180, qr_points, strategy_used)
    
    Args:
        src: BGR image (np.ndarray)
        threshold_value: Threshold cho binary (khÃ´ng dÃ¹ng ná»¯a vÃ¬ Otsu adaptive)
    
    Returns:
        tuple: (rect, box, qr_text, qr_points_180, qr_points, strategy_used)
               hoáº·c (None, None, None, None, None, None) náº¿u tháº¥t báº¡i
    """
    if src is None or src.size == 0:
        return (None, None, None, None, None, None)
    
    try:
        # 1. PREPROCESSING
        gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # 2. Táº¦NG 1: PhÃ¢n tÃ­ch
        analysis = analyze_frame(gray)
        
        # 3. Log analysis
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘           FRAME ANALYSIS - AUTO CONTRAST DETECTION            â•‘")
        print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        print(f"â•‘  ğŸ“Š Final Score:     {analysis.final_score:6.3f}                              â•‘")
        print(f"â•‘  ğŸ¯ Strategy Level:  {analysis.level:<10}                        â•‘")
        print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
        print("â•‘  METRICS BREAKDOWN:                                            â•‘")
        print(f"â•‘    â€¢ Separation:     {analysis.separation:6.3f}  (peaks: {analysis.peak1_position:3}, {analysis.peak2_position:3})       â•‘")
        print(f"â•‘    â€¢ Edge Strength:  {analysis.edge_strength:6.3f}  ({analysis.edge_pixel_count:5} pixels)         â•‘")
        print(f"â•‘    â€¢ Contrast Ratio: {analysis.contrast_ratio:6.3f}  (Ïƒ={analysis.stddev_intensity:6.1f})           â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        # 4. Táº¦NG 2: Routing vÃ  Fallback
        result = None
        strategy_used = ""
        
        if analysis.level == 'High':
            print("ğŸŸ¢ Executing Strategy: HIGH CONTRAST")
            result = detect_with_high_contrast(src, gray, threshold_value)
            print(f"   Result: {'âœ… SUCCESS' if result[0] is not None else 'âŒ FAILED'}")
            if result[0] is not None:
                strategy_used = "HIGH"
        
        elif analysis.level == 'Medium':
            print("ğŸŸ¡ Executing Strategy: MEDIUM CONTRAST")
            result = detect_with_medium_contrast(src, gray)
            print(f"   Result: {'âœ… SUCCESS' if result[0] is not None else 'âŒ FAILED'}")
            
            if result[0] is not None:
                strategy_used = "MEDIUM"
            else:
                # Fallback to HIGH
                print("âš ï¸  MEDIUM failed, falling back to HIGH strategy...")
                result = detect_with_high_contrast(src, gray, threshold_value)
                print(f"   Fallback Result: {'âœ… SUCCESS' if result[0] is not None else 'âŒ FAILED'}")
                if result[0] is not None:
                    strategy_used = "MEDIUMâ†’HIGH"
        
        elif analysis.level == 'Low':
            print("ğŸ”´ Executing Strategy: LOW CONTRAST (QR-First)")
            
            # Apply CLAHE preprocessing (nhÆ° trong C#)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            gray = clahe.apply(gray)
            print("  â†’ Applied CLAHE preprocessing (adaptive contrast enhancement)")
            
            result = detect_with_low_contrast(src, gray)
            print(f"   Result: {'âœ… SUCCESS' if result[0] is not None else 'âŒ FAILED'}")
            
            if result[0] is not None:
                strategy_used = "LOW"
            else:
                # Fallback to MEDIUM
                print("âš ï¸  LOW failed, falling back to MEDIUM strategy...")
                result = detect_with_medium_contrast(src, gray)
                print(f"   Fallback Result: {'âœ… SUCCESS' if result[0] is not None else 'âŒ FAILED'}")
                
                if result[0] is not None:
                    strategy_used = "LOWâ†’MEDIUM"
                else:
                    # Fallback to HIGH
                    print("âš ï¸  MEDIUM failed, falling back to HIGH strategy...")
                    result = detect_with_high_contrast(src, gray, threshold_value)
                    print(f"   Final Fallback Result: {'âœ… SUCCESS' if result[0] is not None else 'âŒ FAILED'}")
                    if result[0] is not None:
                        strategy_used = "LOWâ†’MEDIUMâ†’HIGH"
        
        else:
            print("âŒ ERROR: Unknown contrast level")
            result = (None, None, None, None, None)
            strategy_used = "ERROR"
        
        # 5. Log final result
        if result[0] is not None:
            qr_text = result[2] if result[2] else "N/A"
            print(f"âœ… FINAL RESULT: Label detected | QR: {qr_text} | Strategy: {strategy_used}")
        else:
            print("âŒ FINAL RESULT: Label NOT detected")
        print("")
        
        # 6. Return with strategy_used
        return (*result, strategy_used)
    
    except Exception as e:
        print(f"[DetectLabelRegion ERROR] {e}")
        import traceback
        traceback.print_exc()
        return (None, None, None, None, None, None)
