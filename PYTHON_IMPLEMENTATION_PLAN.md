# üìã K·∫ø ho·∫°ch tri·ªÉn khai Python - Label Region Extractor

## üéØ M·ª•c ti√™u

Chuy·ªÉn ƒë·ªïi logic `LabelRegionExtractor.cs` sang Python ƒë·ªÉ test/debug tr∆∞·ªõc khi √°p d·ª•ng v√†o production C#. Tri·ªÉn khai **ch√≠nh x√°c 100%** logic t·ª´ m√£ ngu·ªìn C# th·ª±c t·∫ø (kh√¥ng theo k·∫ø ho·∫°ch c≈© v√¨ c√≥ s·ª± kh√°c bi·ªát).

---

## üìä Ki·∫øn tr√∫c t·ªïng th·ªÉ: H·ªá th·ªëng 3 t·∫ßng (Theo m√£ C# th·ª±c t·∫ø)

```
·∫¢nh ƒë·∫ßu v√†o (src)
         ‚Üì
    Preprocessing
    (BGR ‚Üí Gray ‚Üí GaussianBlur 5√ó5)
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  T·∫¶NG 1: Ph√¢n t√≠ch t·ª± ƒë·ªông      ‚îÇ
‚îÇ  3 Metrics ‚Üí Final Score        ‚îÇ
‚îÇ  ‚Üí ContrastLevel                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    Ph√¢n lo·∫°i theo Level
         ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚Üì            ‚Üì          ‚Üì
HIGH         MEDIUM       LOW
(>0.45)    (0.25-0.45)  (<0.25)
   ‚Üì            ‚Üì          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇBinary  ‚îÇ  ‚îÇCanny   ‚îÇ  ‚îÇCLAHE + ‚îÇ
‚îÇOtsu    ‚îÇ  ‚îÇ30/100  ‚îÇ  ‚îÇQR-First‚îÇ
‚îÇ+Morph  ‚îÇ  ‚îÇ+Loop   ‚îÇ  ‚îÇGeometry‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚Üì            ‚Üì          ‚Üì
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
     Fallback Chain
  (MEDIUM‚ÜíHIGH, LOW‚ÜíMEDIUM‚ÜíHIGH)
         ‚Üì
  (rect, box, qrText, qrPoints180, qrPoints, strategyUsed)
```

---

## üîß Constants (Theo m√£ C# th·ª±c t·∫ø)

```python
# Analysis thresholds
HIGH_THRESHOLD = 0.45      # N·ªõi r·ªông t·ª´ 0.6 ‚Üí 0.45
MEDIUM_THRESHOLD = 0.25    # Thu h·∫πp t·ª´ 0.3 ‚Üí 0.25
EDGE_MAX = 0.1             # Normalization cho edge strength

# Label expansion ratios (cho Strategy LOW)
LABEL_WIDTH_RATIO = 4.0    # Gi·∫£m t·ª´ 9.0 ‚Üí 4.0
LABEL_HEIGHT_RATIO = 3.0   # Gi·ªØ nguy√™n 3√ó
```

**L∆∞u √Ω:** M√£ C# ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh kh√°c so v·ªõi k·∫ø ho·∫°ch ban ƒë·∫ßu!

---

## üìå T·∫¶NG 1: Analysis Methods

### 1. `analyze_histogram(gray: np.ndarray) -> tuple[int, int, float]`

**Input:** ·∫¢nh grayscale (height, width)  
**Output:** `(peak1_pos, peak2_pos, separation)`

**Chi ti·∫øt implementation:**

```python
def analyze_histogram(gray):
    """
    Ph√¢n t√≠ch histogram ƒë·ªÉ t√¨m 2 peaks ch√≠nh v√† t√≠nh separation.
    
    Logic t·ª´ C#:
    1. L·∫•y v√πng center (1/3 k√≠ch th∆∞·ªõc nh·ªè nh·∫•t)
    2. T√≠nh histogram 256 bins
    3. Smooth b·∫±ng moving average 5 bins
    4. T√¨m local maxima (> 0.5√óavgHeight, c√°ch nhau >30 bins)
    5. Ch·ªçn 2 peaks cao nh·∫•t, sort theo position
    6. separation = |peak2 - peak1| / 255.0
    """
    
    # 1. Center region (1/3 min dimension)
    sample_size = min(gray.shape[1], gray.shape[0]) // 3
    cx = gray.shape[1] // 2
    cy = gray.shape[0] // 2
    x1 = cx - sample_size // 2
    y1 = cy - sample_size // 2
    center_roi = gray[y1:y1+sample_size, x1:x1+sample_size]
    
    # 2. Histogram
    hist = cv2.calcHist([center_roi], [0], None, [256], [0, 256])
    hist = hist.flatten()
    
    # 3. Smooth (moving average 5 bins)
    smoothed = np.copy(hist)
    for i in range(2, 254):
        smoothed[i] = np.mean(hist[i-2:i+3])
    
    # 4. Find local maxima
    avg_height = np.mean(smoothed)
    threshold = avg_height * 0.5
    
    peaks = []
    for i in range(10, 246):
        is_local_max = (smoothed[i] > smoothed[i-1] and 
                       smoothed[i] > smoothed[i+1] and 
                       smoothed[i] > threshold)
        
        if is_local_max:
            # Check not too close to existing peaks
            too_close = any(abs(p[0] - i) < 30 for p in peaks)
            if not too_close:
                peaks.append((i, smoothed[i]))
    
    # 5. Take 2 highest peaks
    if len(peaks) < 2:
        return (0, 255, 0.0)
    
    peaks = sorted(peaks, key=lambda x: x[1], reverse=True)[:2]
    peaks = sorted(peaks, key=lambda x: x[0])  # Sort by position
    
    peak1 = peaks[0][0]
    peak2 = peaks[1][0]
    separation = abs(peak2 - peak1) / 255.0
    
    return (peak1, peak2, separation)
```

---

### 2. `analyze_edges(gray: np.ndarray) -> tuple[int, float]`

**Input:** ·∫¢nh grayscale  
**Output:** `(edge_pixels, edge_strength)`

```python
def analyze_edges(gray):
    """
    Ph√¢n t√≠ch edges b·∫±ng Canny ƒë·ªÉ t√≠nh edge strength.
    
    Logic t·ª´ C#:
    1. L·∫•y v√πng center (1/3 k√≠ch th∆∞·ªõc)
    2. Canny(50, 150)
    3. ƒê·∫øm non-zero pixels
    4. edge_strength = edge_pixels / total_pixels
    """
    
    # 1. Center region
    sample_size = min(gray.shape[1], gray.shape[0]) // 3
    cx = gray.shape[1] // 2
    cy = gray.shape[0] // 2
    x1 = cx - sample_size // 2
    y1 = cy - sample_size // 2
    center_roi = gray[y1:y1+sample_size, x1:x1+sample_size]
    
    # 2. Canny Edge Detection
    edges = cv2.Canny(center_roi, threshold1=50, threshold2=150)
    
    # 3. Count edge pixels
    edge_pixels = cv2.countNonZero(edges)
    total_pixels = center_roi.shape[0] * center_roi.shape[1]
    
    edge_strength = edge_pixels / total_pixels
    
    return (edge_pixels, edge_strength)
```

---

### 3. `analyze_contrast(gray: np.ndarray) -> tuple[float, float, float]`

**Input:** ·∫¢nh grayscale  
**Output:** `(mean, stddev, contrast_ratio)`

```python
def analyze_contrast(gray):
    """
    Ph√¢n t√≠ch contrast b·∫±ng standard deviation.
    
    Logic t·ª´ C#:
    1. L·∫•y v√πng center (1/3 k√≠ch th∆∞·ªõc)
    2. T√≠nh mean, stddev
    3. contrast_ratio = stddev / 128.0
    """
    
    # 1. Center region
    sample_size = min(gray.shape[1], gray.shape[0]) // 3
    cx = gray.shape[1] // 2
    cy = gray.shape[0] // 2
    x1 = cx - sample_size // 2
    y1 = cy - sample_size // 2
    center_roi = gray[y1:y1+sample_size, x1:x1+sample_size]
    
    # 2. Calculate mean and stddev
    mean, stddev = cv2.meanStdDev(center_roi)
    mean = mean[0][0]
    stddev = stddev[0][0]
    
    # 3. Contrast ratio
    contrast_ratio = stddev / 128.0
    
    return (mean, stddev, contrast_ratio)
```

---

### 4. `analyze_frame(gray: np.ndarray) -> ContrastAnalysisResult`

**Input:** ·∫¢nh grayscale  
**Output:** Dataclass ch·ª©a level + metrics

```python
@dataclass
class ContrastAnalysisResult:
    """K·∫øt qu·∫£ ph√¢n t√≠ch ƒë·ªô t∆∞∆°ng ph·∫£n."""
    level: str  # 'High', 'Medium', 'Low'
    final_score: float
    
    # 3 metrics
    separation: float
    edge_strength: float
    contrast_ratio: float
    
    # Debug info
    peak1_position: int
    peak2_position: int
    edge_pixel_count: int
    mean_intensity: float
    stddev_intensity: float


def analyze_frame(gray):
    """
    Ph√¢n t√≠ch frame ƒë·ªÉ t√≠nh final score v√† x√°c ƒë·ªãnh contrast level.
    
    Logic t·ª´ C#:
    1. G·ªçi 3 h√†m ph√¢n t√≠ch
    2. Normalize edge_strength (min(edge_strength / 0.1, 1.0))
    3. Final Score = separation√ó0.4 + edge_strength_norm√ó0.3 + contrast_ratio√ó0.3
    4. Ph√¢n lo·∫°i: >0.45=High, 0.25-0.45=Medium, <0.25=Low
    """
    
    # 1. Call 3 analysis methods
    peak1, peak2, separation = analyze_histogram(gray)
    edge_pixels, edge_strength = analyze_edges(gray)
    mean, stddev, contrast_ratio = analyze_contrast(gray)
    
    # 2. Normalize edge strength
    edge_strength_norm = min(edge_strength / EDGE_MAX, 1.0)
    
    # 3. Calculate Final Score
    final_score = (separation * 0.4 + 
                   edge_strength_norm * 0.3 + 
                   contrast_ratio * 0.3)
    
    # 4. Determine level
    if final_score > HIGH_THRESHOLD:
        level = 'High'
    elif final_score > MEDIUM_THRESHOLD:
        level = 'Medium'
    else:
        level = 'Low'
    
    # 5. Return result
    return ContrastAnalysisResult(
        level=level,
        final_score=final_score,
        separation=separation,
        edge_strength=edge_strength,
        contrast_ratio=contrast_ratio,
        peak1_position=peak1,
        peak2_position=peak2,
        edge_pixel_count=edge_pixels,
        mean_intensity=mean,
        stddev_intensity=stddev
    )
```

---

## üìå T·∫¶NG 2: Strategy Methods

### Strategy HIGH: Binary Threshold + Morphology

```python
def detect_with_high_contrast(src, gray, threshold_value=150):
    """
    Strategy HIGH: Binary Threshold + Morphology (cho √°o t·ªëi/m√†u ƒë·∫≠m).
    
    Logic t·ª´ C# (ƒë√£ c·∫≠p nh·∫≠t):
    1. Otsu adaptive threshold thay v√¨ hardcoded value
    2. Morphology: Open(3√ó3, 1 iter) ‚Üí Close(3√ó3, 2 iters)
    3. FindContours(EXTERNAL)
    4. Ch·ªçn largest contour theo area
    5. MinAreaRect ‚Üí box
    6. Crop bounding rect ‚Üí QR verification
    7. Tr·∫£ v·ªÅ (rect, box, qr_text, qr_points_180, qr_points) ho·∫∑c (None, None, None, None, None)
    
    Returns:
        tuple: (RotatedRect dict, box points, qr_text, qr_points_180, qr_points)
    """
    
    print("  ‚Üí Method: Binary Threshold + Morphology")
    
    # 1. Otsu adaptive threshold
    _, binary = cv2.threshold(gray, 0, 255, 
                              cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    otsu_threshold = _  # OpenCV returns threshold value
    print(f"  ‚Üí Otsu adaptive threshold: {otsu_threshold:.1f}")
    
    # 2. Morphology
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    morph = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)
    morph = cv2.morphologyEx(morph, cv2.MORPH_CLOSE, kernel, iterations=2)
    
    # 3. Find contours
    contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, 
                                   cv2.CHAIN_APPROX_SIMPLE)
    
    if len(contours) == 0:
        print("  ‚úó No contours found")
        return (None, None, None, None, None)
    
    print(f"  ‚Üí Found {len(contours)} contours")
    
    # 4. Find largest contour
    biggest = max(contours, key=cv2.contourArea)
    max_area = cv2.contourArea(biggest)
    print(f"  ‚Üí Largest contour area: {max_area:.0f} pixels")
    
    # 5. MinAreaRect
    rect = cv2.minAreaRect(biggest)
    box = cv2.boxPoints(rect)
    box = np.int0(box)
    
    # 6. Crop and verify QR
    bound = cv2.boundingRect(biggest)
    x, y, w, h = bound
    
    # Clamp to image bounds
    x = max(0, x)
    y = max(0, y)
    w = min(src.shape[1] - x, w)
    h = min(src.shape[0] - y, h)
    
    label_roi = src[y:y+h, x:x+w]
    
    # QR detection
    qr_detector = cv2.QRCodeDetector()
    qr_text, qr_points_180, _ = qr_detector.detectAndDecode(label_roi)
    
    qr_points = None
    if qr_points_180 is not None:
        # Convert to global coordinates
        qr_points = qr_points_180.copy()
        qr_points[:, 0] += x
        qr_points[:, 1] += y
    
    if qr_text:
        print(f"  ‚úì QR detected: {qr_text}")
        return (rect, box, qr_text, qr_points_180, qr_points)
    
    print("  ‚úó No QR code found in label region")
    return (None, None, None, None, None)
```

---

### Strategy MEDIUM: Canny Edge Detection

```python
def detect_with_medium_contrast(src, gray):
    """
    Strategy MEDIUM: Canny Edge + Strong Morphology (cho √°o m√†u nh·∫°t).
    
    Logic t·ª´ C# (ƒë√£ c·∫≠p nh·∫≠t):
    1. Canny(30, 100) - Lower thresholds ƒë·ªÉ nh·∫°y h∆°n
    2. Morphology: Close(7√ó7, 3 iters) ‚Üí Dilate(7√ó7, 1 iter)
    3. FindContours(EXTERNAL)
    4. Filter CH·ªà theo area ratio (5-80%)
    5. Sort theo area (l·ªõn nh·∫•t tr∆∞·ªõc)
    6. Loop candidates ‚Üí verify QR ‚Üí Early exit khi t√¨m th·∫•y
    
    Returns:
        tuple: (RotatedRect dict, box points, qr_text, qr_points_180, qr_points)
    """
    
    print("  ‚Üí Method: Canny Edge + Strong Morphology")
    
    # 1. Canny with lower thresholds
    edges = cv2.Canny(gray, threshold1=30, threshold2=100)
    print("  ‚Üí Lower Canny thresholds (30/100) for better edge detection")
    
    # 2. Strong morphology
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))
    edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=3)
    edges = cv2.dilate(edges, kernel, iterations=1)
    
    # 3. Find contours
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, 
                                   cv2.CHAIN_APPROX_SIMPLE)
    
    if len(contours) == 0:
        print("  ‚úó No contours found")
        return (None, None, None, None, None)
    
    print(f"  ‚Üí Found {len(contours)} contours")
    
    # 4. Filter by area ratio (5-80%)
    roi_area = gray.shape[0] * gray.shape[1]
    candidates = []
    
    for c in contours:
        area = cv2.contourArea(c)
        area_ratio = area / roi_area
        
        if 0.05 <= area_ratio <= 0.80:
            rect = cv2.minAreaRect(c)
            candidates.append((c, rect, area))
    
    # 5. Sort by area (largest first)
    candidates = sorted(candidates, key=lambda x: x[2], reverse=True)
    print(f"  ‚Üí {len(candidates)} candidates after filtering (area 5-80%)")
    
    # 6. Loop and verify QR (Early Exit)
    for contour, rect, area in candidates:
        bound = cv2.boundingRect(contour)
        x, y, w, h = bound
        
        # Clamp to bounds
        x = max(0, x)
        y = max(0, y)
        w = min(src.shape[1] - x, w)
        h = min(src.shape[0] - y, h)
        
        if w <= 0 or h <= 0:
            continue
        
        label_roi = src[y:y+h, x:x+w]
        
        # QR detection
        qr_detector = cv2.QRCodeDetector()
        qr_text, qr_points_180, _ = qr_detector.detectAndDecode(label_roi)
        
        qr_points = None
        if qr_points_180 is not None:
            qr_points = qr_points_180.copy()
            qr_points[:, 0] += x
            qr_points[:, 1] += y
        
        if qr_text:
            box = cv2.boxPoints(rect)
            box = np.int0(box)
            print(f"  ‚úì QR detected in candidate (area={area:.0f}): {qr_text}")
            return (rect, box, qr_text, qr_points_180, qr_points)
    
    print("  ‚úó No QR found in any candidate")
    return (None, None, None, None, None)
```

---

### Strategy LOW: QR-First Geometry

```python
def detect_with_low_contrast(src, gray):
    """
    Strategy LOW: QR-First + Geometry Inference (cho √°o tr·∫Øng/kem).
    
    Logic t·ª´ C# (ƒë√£ c·∫≠p nh·∫≠t):
    1. Apply CLAHE preprocessing ƒë·ªÉ enhance contrast c·ª•c b·ªô
    2. Histogram equalization cho QR detection robustness
    3. Detect QR tr√™n enhanced image (fallback to original n·∫øu fail)
    4. T√≠nh geometry QR (vectors, width, height, angle)
    5. Suy lu·∫≠n label v·ªõi expansion ratios (4.0√ó, 3.0√ó)
    6. Construct 4 corners (QR ·ªü TR√ÅI D∆Ø·ªöI, expand PH·∫¢I + TR√äN)
    7. T·∫°o RotatedRect t·ª´ 4 corners
    
    Returns:
        tuple: (RotatedRect dict, box points, qr_text, None, qr_points)
        Note: qr_points_180 = None v√¨ detect tr√™n to√†n ·∫£nh
    """
    
    print("  ‚Üí Method: QR-First + Geometry Inference")
    
    # 1. CLAHE preprocessing (ƒë√£ ƒë∆∞·ª£c apply tr∆∞·ªõc khi g·ªçi h√†m n√†y trong C#)
    # Note: Trong C# CLAHE ƒë∆∞·ª£c apply ·ªü switch-case, kh√¥ng trong h√†m
    # Nh∆∞ng ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ƒë·ªìng nh·∫•t, ta √°p d·ª•ng l·∫°i ·ªü ƒë√¢y
    
    # 2. Enhance contrast cho QR detection
    enhanced = cv2.equalizeHist(gray)
    print("  ‚Üí Applied histogram equalization for QR detection robustness")
    
    # 3. Detect QR
    qr_detector = cv2.QRCodeDetector()
    
    # Try on enhanced first
    qr_text, qr_points, _ = qr_detector.detectAndDecode(enhanced)
    
    # Fallback to original
    if not qr_text:
        qr_text, qr_points, _ = qr_detector.detectAndDecode(src)
    
    if not qr_text or qr_points is None or len(qr_points) < 4:
        print("  ‚úó No QR code detected")
        return (None, None, None, None, None)
    
    print(f"  ‚úì QR detected: {qr_text}")
    
    # 4. Calculate QR geometry
    p0 = qr_points[0]  # top-left
    p1 = qr_points[1]  # top-right
    p3 = qr_points[3]  # bottom-left
    
    top_vec = p1 - p0
    left_vec = p3 - p0
    
    qr_width = np.linalg.norm(top_vec)
    qr_height = np.linalg.norm(left_vec)
    
    # Unit vectors
    dir_right = top_vec / qr_width
    dir_down = left_vec / qr_height
    dir_left = -dir_right
    
    angle_rad = np.arctan2(top_vec[1], top_vec[0])
    angle_deg = angle_rad * 180.0 / np.pi
    print(f"  ‚Üí QR geometry: {qr_width:.1f}√ó{qr_height:.1f} px, angle={angle_deg:.1f}¬∞")
    
    # 5. Infer label dimensions
    label_width = qr_width * LABEL_WIDTH_RATIO
    label_height = qr_height * LABEL_HEIGHT_RATIO
    print(f"  ‚Üí Predicted label: {label_width:.1f}√ó{label_height:.1f} px")
    print(f"  ‚Üí Expansion: width={LABEL_WIDTH_RATIO}√óQR, height={LABEL_HEIGHT_RATIO}√óQR")
    
    # 6. Calculate 4 corners
    # QR ·ªü TR√ÅI D∆Ø·ªöI c·ªßa label ‚Üí expand PH·∫¢I v√† L√äN TR√äN
    qr_top_left = p0
    
    # Label top-left: ƒëi l√™n tr√™n t·ª´ QR top-left
    label_top_left = qr_top_left - dir_down * (label_height - qr_height)
    
    # Label top-right: t·ª´ top-left ƒëi sang ph·∫£i
    label_top_right = label_top_left + dir_right * label_width
    
    # Label bottom-left: t·ª´ top-left ƒëi xu·ªëng
    label_bottom_left = label_top_left + dir_down * label_height
    
    # Label bottom-right: t·ª´ bottom-left ƒëi sang ph·∫£i
    label_bottom_right = label_bottom_left + dir_right * label_width
    
    # 7. Create RotatedRect
    label_center = (label_top_left + label_top_right + 
                   label_bottom_right + label_bottom_left) / 4.0
    
    angle = angle_deg
    
    # RotatedRect as dict (Python doesn't have C# RotatedRect)
    rect = {
        'center': tuple(label_center),
        'size': (label_width, label_height),
        'angle': angle
    }
    
    box = np.array([label_top_left, label_top_right, 
                    label_bottom_right, label_bottom_left], dtype=np.int32)
    
    print(f"  ‚úì Label constructed: center=({label_center[0]:.1f},{label_center[1]:.1f}), angle={angle:.1f}¬∞")
    
    # qr_points_180 = None (detect tr√™n to√†n ·∫£nh, kh√¥ng c√≥ ROI c·ª•c b·ªô)
    return (rect, box, qr_text, None, qr_points)
```

---

## üìå H√†m ch√≠nh: detect_label_region()

```python
def detect_label_region(src, threshold_value=150):
    """
    Ph√°t hi·ªán v√πng nh√£n trong ·∫£nh.
    
    Logic t·ª´ C# (flow ch√≠nh x√°c):
    1. Preprocessing: BGR ‚Üí Gray ‚Üí GaussianBlur(5√ó5)
    2. T·∫¶NG 1: Ph√¢n t√≠ch (AnalyzeFrame) ‚Üí ContrastAnalysisResult
    3. Log analysis metrics
    4. T·∫¶NG 2: Routing theo level:
       - HIGH: DetectWithHighContrast()
       - MEDIUM: DetectWithMediumContrast() ‚Üí fallback to HIGH
       - LOW: Apply CLAHE ‚Üí DetectWithLowContrast() ‚Üí fallback chain (‚ÜíMEDIUM‚ÜíHIGH)
    5. Tr·∫£ v·ªÅ (rect, box, qr_text, qr_points_180, qr_points, strategy_used)
    
    Args:
        src: BGR image (np.ndarray)
        threshold_value: Threshold cho binary (kh√¥ng d√πng n·ªØa v√¨ Otsu adaptive)
    
    Returns:
        tuple: (rect, box, qr_text, qr_points_180, qr_points, strategy_used)
               ho·∫∑c (None, None, None, None, None, None) n·∫øu th·∫•t b·∫°i
    """
    
    if src is None or src.size == 0:
        return (None, None, None, None, None, None)
    
    try:
        # 1. PREPROCESSING
        gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (5, 5), 0)
        
        # 2. T·∫¶NG 1: Ph√¢n t√≠ch
        analysis = analyze_frame(gray)
        
        # 3. Log analysis
        print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
        print("‚ïë           FRAME ANALYSIS - AUTO CONTRAST DETECTION            ‚ïë")
        print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")
        print(f"‚ïë  üìä Final Score:     {analysis.final_score:6.3f}                              ‚ïë")
        print(f"‚ïë  üéØ Strategy Level:  {analysis.level:<10}                        ‚ïë")
        print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£")
        print("‚ïë  METRICS BREAKDOWN:                                            ‚ïë")
        print(f"‚ïë    ‚Ä¢ Separation:     {analysis.separation:6.3f}  (peaks: {analysis.peak1_position:3}, {analysis.peak2_position:3})       ‚ïë")
        print(f"‚ïë    ‚Ä¢ Edge Strength:  {analysis.edge_strength:6.3f}  ({analysis.edge_pixel_count:5} pixels)         ‚ïë")
        print(f"‚ïë    ‚Ä¢ Contrast Ratio: {analysis.contrast_ratio:6.3f}  (œÉ={analysis.stddev_intensity:6.1f})           ‚ïë")
        print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
        
        # 4. T·∫¶NG 2: Routing v√† Fallback
        result = None
        strategy_used = ""
        
        if analysis.level == 'High':
            print("üü¢ Executing Strategy: HIGH CONTRAST")
            result = detect_with_high_contrast(src, gray, threshold_value)
            print(f"   Result: {'‚úÖ SUCCESS' if result[0] is not None else '‚ùå FAILED'}")
            if result[0] is not None:
                strategy_used = "HIGH"
        
        elif analysis.level == 'Medium':
            print("üü° Executing Strategy: MEDIUM CONTRAST")
            result = detect_with_medium_contrast(src, gray)
            print(f"   Result: {'‚úÖ SUCCESS' if result[0] is not None else '‚ùå FAILED'}")
            
            if result[0] is not None:
                strategy_used = "MEDIUM"
            else:
                # Fallback to HIGH
                print("‚ö†Ô∏è  MEDIUM failed, falling back to HIGH strategy...")
                result = detect_with_high_contrast(src, gray, threshold_value)
                print(f"   Fallback Result: {'‚úÖ SUCCESS' if result[0] is not None else '‚ùå FAILED'}")
                if result[0] is not None:
                    strategy_used = "MEDIUM‚ÜíHIGH"
        
        elif analysis.level == 'Low':
            print("üî¥ Executing Strategy: LOW CONTRAST (QR-First)")
            
            # Apply CLAHE preprocessing (nh∆∞ trong C#)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            gray = clahe.apply(gray)
            print("  ‚Üí Applied CLAHE preprocessing (adaptive contrast enhancement)")
            
            result = detect_with_low_contrast(src, gray)
            print(f"   Result: {'‚úÖ SUCCESS' if result[0] is not None else '‚ùå FAILED'}")
            
            if result[0] is not None:
                strategy_used = "LOW"
            else:
                # Fallback to MEDIUM
                print("‚ö†Ô∏è  LOW failed, falling back to MEDIUM strategy...")
                result = detect_with_medium_contrast(src, gray)
                print(f"   Fallback Result: {'‚úÖ SUCCESS' if result[0] is not None else '‚ùå FAILED'}")
                
                if result[0] is not None:
                    strategy_used = "LOW‚ÜíMEDIUM"
                else:
                    # Fallback to HIGH
                    print("‚ö†Ô∏è  MEDIUM failed, falling back to HIGH strategy...")
                    result = detect_with_high_contrast(src, gray, threshold_value)
                    print(f"   Final Fallback Result: {'‚úÖ SUCCESS' if result[0] is not None else '‚ùå FAILED'}")
                    if result[0] is not None:
                        strategy_used = "LOW‚ÜíMEDIUM‚ÜíHIGH"
        
        else:
            print("‚ùå ERROR: Unknown contrast level")
            result = (None, None, None, None, None)
            strategy_used = "ERROR"
        
        # 5. Log final result
        if result[0] is not None:
            qr_text = result[2] if result[2] else "N/A"
            print(f"‚úÖ FINAL RESULT: Label detected | QR: {qr_text} | Strategy: {strategy_used}")
        else:
            print("‚ùå FINAL RESULT: Label NOT detected")
        print("")
        
        # 6. Return with strategy_used
        return (*result, strategy_used)
    
    except Exception as e:
        print(f"[DetectLabelRegion ERROR] {e}")
        import traceback
        traceback.print_exc()
        return (None, None, None, None, None, None)
```

---

## üìÇ C·∫•u tr√∫c th∆∞ m·ª•c Python project

```
label-detector/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ label_region_extractor.py    # Core logic (T·∫¶NG 1 + T·∫¶NG 2)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                      # Helper functions (visualization, etc.)
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_label_detector.py       # Unit tests
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ test_images/                 # ·∫¢nh test
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ black_shirt.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ white_shirt.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gray_shirt.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ results/                     # Output visualization
‚îÇ
‚îú‚îÄ‚îÄ main.py                          # Script ch·∫°y test tr√™n ·∫£nh
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencies
‚îî‚îÄ‚îÄ PYTHON_IMPLEMENTATION_PLAN.md    # File n√†y
```

---

## üîß Dependencies (requirements.txt)

```
opencv-python>=4.8.0
opencv-contrib-python>=4.8.0  # For QRCodeDetector
numpy>=1.24.0
```

---

## üß™ Test Script (main.py)

```python
import cv2
import numpy as np
from pathlib import Path
from src.label_region_extractor import detect_label_region

def visualize_result(src, result):
    """V·∫Ω k·∫øt qu·∫£ l√™n ·∫£nh."""
    rect, box, qr_text, qr_points_180, qr_points, strategy_used = result
    
    if rect is None:
        # Draw "NOT DETECTED" text
        vis = src.copy()
        cv2.putText(vis, "NOT DETECTED", (50, 50),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)
        return vis
    
    vis = src.copy()
    
    # Draw label box
    cv2.drawContours(vis, [box], 0, (0, 255, 0), 2)
    
    # Draw QR points
    if qr_points is not None:
        for i, pt in enumerate(qr_points):
            pt = tuple(pt.astype(int))
            cv2.circle(vis, pt, 5, (0, 0, 255), -1)
            cv2.putText(vis, str(i), pt, 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
    
    # Draw info text
    info = f"Strategy: {strategy_used} | QR: {qr_text if qr_text else 'N/A'}"
    cv2.putText(vis, info, (10, 30), 
               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    
    return vis


def main():
    """Test detection tr√™n ·∫£nh m·∫´u."""
    
    # Load test image
    image_path = "data/test_images/test.jpg"
    src = cv2.imread(image_path)
    
    if src is None:
        print(f"‚ùå Cannot load image: {image_path}")
        return
    
    print(f"‚úÖ Loaded image: {src.shape}")
    print("")
    
    # Run detection
    result = detect_label_region(src)
    
    # Visualize
    vis = visualize_result(src, result)
    
    # Save result
    output_path = "data/results/output.jpg"
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    cv2.imwrite(output_path, vis)
    print(f"‚úÖ Saved result to: {output_path}")
    
    # Display
    cv2.imshow("Detection Result", vis)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
```

---

## üìä Chi ti·∫øt kh√°c bi·ªát so v·ªõi k·∫ø ho·∫°ch ban ƒë·∫ßu

### 1. **Thresholds ƒë√£ ƒëi·ªÅu ch·ªânh**
| Parameter | K·∫ø ho·∫°ch c≈© | M√£ C# th·ª±c t·∫ø | L√Ω do |
|-----------|-------------|---------------|--------|
| HIGH_THRESHOLD | 0.6 | 0.45 | N·ªõi r·ªông ƒë·ªÉ √°o ƒëen v√†o HIGH |
| MEDIUM_THRESHOLD | 0.3 | 0.25 | Thu h·∫πp ƒë·ªÉ tr√°nh overlap |
| LABEL_WIDTH_RATIO | 9.0 | 4.0 | Frame kh√¥ng qu√° d√†i |

### 2. **Strategy HIGH: Otsu Threshold**
- K·∫ø ho·∫°ch: Hardcoded threshold (150)
- Th·ª±c t·∫ø: **Otsu adaptive threshold** (t·ª± ƒë·ªông)
- L·ª£i √≠ch: Th√≠ch ·ª©ng v·ªõi m·ªçi m√†u √°o (ƒëen, tr·∫Øng, x√°m...)

### 3. **Strategy MEDIUM: Canny thresholds**
- K·∫ø ho·∫°ch: 50/150
- Th·ª±c t·∫ø: **30/100** (lower thresholds)
- L√Ω do: Nh·∫°y h∆°n v·ªõi edge tr√™n n·ªÅn t·ªëi/s√°ng

### 4. **Strategy LOW: CLAHE preprocessing**
- K·∫ø ho·∫°ch: Ch·ªâ c√≥ Histogram Equalization
- Th·ª±c t·∫ø: **CLAHE (Contrast Limited Adaptive Histogram Equalization)**
- L·ª£i √≠ch: C·∫£i thi·ªán contrast c·ª•c b·ªô, ho·∫°t ƒë·ªông t·ªët v·ªõi m·ªçi m√†u √°o

### 5. **Fallback Chain ƒë·∫ßy ƒë·ªß**
- HIGH: Kh√¥ng c√≥ fallback
- MEDIUM: ‚Üí HIGH
- LOW: ‚Üí MEDIUM ‚Üí HIGH (2-level fallback)

---

## üöÄ L·ªô tr√¨nh tri·ªÉn khai

### Phase 1: Setup project (30 ph√∫t)
- [x] T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c
- [ ] T·∫°o `requirements.txt`
- [ ] T·∫°o file `__init__.py`
- [ ] Setup environment (`python -m venv venv`)

### Phase 2: Implement T·∫¶NG 1 (2 gi·ªù)
- [ ] T·∫°o `ContrastAnalysisResult` dataclass
- [ ] Implement `analyze_histogram()`
- [ ] Implement `analyze_edges()`
- [ ] Implement `analyze_contrast()`
- [ ] Implement `analyze_frame()`
- [ ] Test v·ªõi 1 ·∫£nh m·∫´u, ki·ªÉm tra metrics

### Phase 3: Implement Strategy HIGH (1 gi·ªù)
- [ ] Implement `detect_with_high_contrast()`
- [ ] Test v·ªõi ·∫£nh √°o ƒëen

### Phase 4: Implement Strategy MEDIUM (1.5 gi·ªù)
- [ ] Implement `detect_with_medium_contrast()`
- [ ] Test v·ªõi ·∫£nh √°o m√†u nh·∫°t

### Phase 5: Implement Strategy LOW (2 gi·ªù)
- [ ] Implement `detect_with_low_contrast()`
- [ ] Test v·ªõi ·∫£nh √°o tr·∫Øng/kem

### Phase 6: T√≠ch h·ª£p (1 gi·ªù)
- [ ] Implement `detect_label_region()` v·ªõi routing + fallback
- [ ] Test end-to-end v·ªõi 10 ·∫£nh

### Phase 7: Visualization & Debug (1 gi·ªù)
- [ ] Implement `visualize_result()`
- [ ] T·∫°o `main.py`
- [ ] Test batch processing

**T·ªïng th·ªùi gian:** ~9 gi·ªù

---

## üêõ Debugging Tips

### 1. Ki·ªÉm tra metrics kh√¥ng ·ªïn ƒë·ªãnh
- In ra histogram smoothed array
- Visualize Canny edges
- Plot metrics qua nhi·ªÅu frames

### 2. Strategy detection sai
- Log t·ª´ng b∆∞·ªõc trong strategies
- Visualize binary/morph/edges
- Ki·ªÉm tra contour filtering

### 3. QR detection fail
- Ki·ªÉm tra ROI crop c√≥ ƒë√∫ng kh√¥ng
- Test QR detector ri√™ng
- Th·ª≠ preprocessing kh√°c (CLAHE, equalizeHist)

### 4. Geometry inference sai (Strategy LOW)
- In ra QR points
- Visualize vectors (dir_right, dir_down)
- Ki·ªÉm tra LABEL_WIDTH_RATIO

---

## üìù Notes quan tr·ªçng

### 1. **RotatedRect trong Python**
OpenCV Python kh√¥ng c√≥ struct RotatedRect nh∆∞ C#. Thay v√†o ƒë√≥:
```python
# C#: RotatedRect rect = Cv2.MinAreaRect(contour);
# Python: rect = cv2.minAreaRect(contour)
#         ‚Üí returns tuple: ((cx, cy), (w, h), angle)

rect = cv2.minAreaRect(contour)
box = cv2.boxPoints(rect)  # Convert to 4 corners
box = np.int0(box)
```

### 2. **QRCodeDetector output**
```python
# C#: qr.DetectAndDecode(image, out points, straight)
# Python: text, points, straight = qr.detectAndDecode(image)

# points shape: (4, 2) or (1, 4, 2) - c·∫ßn reshape!
if points is not None and points.ndim == 3:
    points = points.reshape(-1, 2)
```

### 3. **Logging format**
Gi·ªØ nguy√™n format box-drawing characters (‚ïî‚ïê‚ïó‚ïë‚ïö‚ïù) ƒë·ªÉ d·ªÖ ƒë·ªëi chi·∫øu v·ªõi C#.

### 4. **Coordinate systems**
- C# OpenCvSharp: `Point(x, y)`
- Python OpenCV: `(x, y)` tuple ho·∫∑c `np.array([x, y])`
- L∆∞u √Ω: NumPy array indexing: `img[y, x]` (row, col)

---

## ‚úÖ Checklist ho√†n th√†nh

### Core Implementation
- [ ] T·∫¶NG 1: 4 h√†m ph√¢n t√≠ch
- [ ] Strategy HIGH
- [ ] Strategy MEDIUM
- [ ] Strategy LOW
- [ ] H√†m ch√≠nh `detect_label_region()`
- [ ] Fallback chain logic

### Testing
- [ ] Test t·ª´ng strategy ri√™ng l·∫ª
- [ ] Test fallback chain
- [ ] Test v·ªõi 30 ·∫£nh (10 m·ªói lo·∫°i √°o)
- [ ] ƒêo accuracy

### Visualization
- [ ] V·∫Ω label box
- [ ] V·∫Ω QR points
- [ ] Display metrics
- [ ] Save results

### Documentation
- [ ] Docstrings cho m·ªçi h√†m
- [ ] Comments gi·∫£i th√≠ch logic ph·ª©c t·∫°p
- [ ] README.md h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

---

**Phi√™n b·∫£n:** 1.0 (From C# Source Code)  
**Ng√†y:** November 13, 2025  
**Tr·∫°ng th√°i:** Ready for Implementation  
**Ngu·ªìn:** `LabelRegionExtractor.cs` (production code)
